{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df6a9d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local environment\n",
      "Root dir: c:\\Users\\patri\\VScodeProjects\\25-ID2223-mlfs-book\n",
      "Added the following directory to the PYTHONPATH: c:\\Users\\patri\\VScodeProjects\\25-ID2223-mlfs-book\n",
      "HopsworksSettings initialized!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", module=\"IPython\")\n",
    "\n",
    "def is_google_colab() -> bool:\n",
    "    if \"google.colab\" in str(get_ipython()):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def clone_repository() -> None:\n",
    "    !git clone https://github.com/featurestorebook/mlfs-book.git\n",
    "    %cd mlfs-book\n",
    "\n",
    "def install_dependencies() -> None:\n",
    "    !pip install --upgrade uv\n",
    "    !uv pip install --all-extras --system --requirement pyproject.toml\n",
    "\n",
    "if is_google_colab():\n",
    "    clone_repository()\n",
    "    install_dependencies()\n",
    "    root_dir = str(Path().absolute())\n",
    "    print(\"Google Colab environment\")\n",
    "else:\n",
    "    root_dir = Path().absolute()\n",
    "    # Strip ~/notebooks/fingrid from PYTHON_PATH if notebook started in one of these subdirectories\n",
    "    if root_dir.parts[-1:] == ('fingrid',):\n",
    "        root_dir = Path(*root_dir.parts[:-1])\n",
    "    if root_dir.parts[-1:] == ('notebooks',):\n",
    "        root_dir = Path(*root_dir.parts[:-1])\n",
    "    root_dir = str(root_dir) \n",
    "    print(\"Local environment\")\n",
    "\n",
    "print(f\"Root dir: {root_dir}\")\n",
    "\n",
    "# Add the root directory to the `PYTHONPATH` \n",
    "if root_dir not in sys.path:\n",
    "    sys.path.append(root_dir)\n",
    "    print(f\"Added the following directory to the PYTHONPATH: {root_dir}\")\n",
    "\n",
    "# Set the environment variables from the file <root_dir>/.env\n",
    "from mlfs import config\n",
    "settings = config.HopsworksSettings(_env_file=f\"{root_dir}/.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e51319b",
   "metadata": {},
   "source": [
    "<span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 01: Feature Backfill for Fingrid Energy Consumption Data</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77395dcc",
   "metadata": {},
   "source": [
    "### <span style='color:#ff5f27'> Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2488b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hopsworks\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define today's date\n",
    "today = datetime.today()\n",
    "\n",
    "# Get API keys from settings\n",
    "FINGRID_API_KEY = settings.FINGRID_KEY\n",
    "if FINGRID_API_KEY is None:\n",
    "    print(\"Warning: FINGRID_KEY not found in .env file. You'll need it to fetch energy data.\")\n",
    "else:\n",
    "    # Convert SecretStr to plain string for use with requests library\n",
    "    FINGRID_API_KEY = FINGRID_API_KEY.get_secret_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4e3ecc",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> STEP 1: Get your Fingrid API Token and Store it in .env file</span>\n",
    "\n",
    "You need to get your Fingrid API key from https://data.fingrid.fi/en/\n",
    "\n",
    "Once you have your API key, save it to the .env file in the root directory of your project:\n",
    "\n",
    " * mv .env.example .env\n",
    " * edit .env\n",
    "\n",
    "In the .env file, add or update:\n",
    "\n",
    "`FINGRID_KEY=\"put your Fingrid API KEY value in this string\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f2a0fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-01 20:37:40,579 INFO: Initializing external client\n",
      "2026-01-01 20:37:40,581 INFO: Base URL: https://c.app.hopsworks.ai:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-01 20:37:43,327 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1286359\n"
     ]
    }
   ],
   "source": [
    "project = hopsworks.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fe50e9",
   "metadata": {},
   "source": [
    "## Hopsworks API Key\n",
    "\n",
    "You need to have registered an account on app.hopsworks.ai.\n",
    "\n",
    "Save the HOPSWORKS_API_KEY to the .env file in the root directory of your project:\n",
    "\n",
    " * mv .env.example .env\n",
    " * edit .env\n",
    "\n",
    "In the .env file, update HOPSWORKS_API_KEY:\n",
    "\n",
    "`HOPSWORKS_API_KEY=\"put API KEY value in this string\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12349995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country: Finland\n",
      "Weather measurement location: Helsinki (60.1699, 24.9384)\n",
      "Backfill period: 2024-01-02 to 2026-01-01\n",
      "Local data directory: c:\\Users\\patri\\VScodeProjects\\25-ID2223-mlfs-book/data\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "FINGRID_BASE_URL = \"https://data.fingrid.fi/api/datasets\"\n",
    "DATASET_ID = \"193\"  # Electricity consumption in Finland (MW)\n",
    "\n",
    "# Weather location (Helsinki coordinates as representative measurement point for Finland)\n",
    "COUNTRY = \"Finland\"\n",
    "CITY = \"Helsinki\"\n",
    "LATITUDE = 60.1699\n",
    "LONGITUDE = 24.9384\n",
    "\n",
    "# Backfill configuration - how many days of historical data to download\n",
    "BACKFILL_DAYS = 730  # 2 years of data for complete seasonal coverage\n",
    "end_date = today\n",
    "start_date = end_date - timedelta(days=BACKFILL_DAYS)\n",
    "\n",
    "# Local storage configuration\n",
    "DATA_DIR = f\"{root_dir}/data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Country: {COUNTRY}\")\n",
    "print(f\"Weather measurement location: {CITY} ({LATITUDE}, {LONGITUDE})\")\n",
    "print(f\"Backfill period: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "print(f\"Local data directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e03e65",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <span style='color:#ff5f27'> STEP 2: Download Historical Energy Consumption Data from Fingrid</span>\n",
    "\n",
    "We will fetch historical electricity consumption data from Fingrid's open data API.\n",
    "The data is measured every 3 hours and represents Finland's nationwide electricity consumption in megawatts (MW)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5a5adf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fetching functions defined\n"
     ]
    }
   ],
   "source": [
    "def fetch_fingrid_data(dataset_id, start_date, end_date, api_key):\n",
    "    \"\"\"\n",
    "    Fetch historical data from Fingrid API.\n",
    "    \n",
    "    Args:\n",
    "        dataset_id: Fingrid dataset ID\n",
    "        start_date: Start date (datetime)\n",
    "        end_date: End date (datetime)\n",
    "        api_key: Fingrid API key\n",
    "    \n",
    "    Returns:\n",
    "        pandas DataFrame with timestamp and value columns\n",
    "    \"\"\"\n",
    "    url = f\"{FINGRID_BASE_URL}/{dataset_id}/data\"\n",
    "    \n",
    "    params = {\n",
    "        \"startTime\": start_date.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "        \"endTime\": end_date.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "        \"format\": \"json\",\n",
    "        \"pageSize\": 20000  # Max records per request\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        \"x-api-key\": api_key,\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    print(f\"Fetching energy data from {params['startTime']} to {params['endTime']}...\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, params=params, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        if 'data' in data:\n",
    "            df = pd.DataFrame(data['data'])\n",
    "            print(f\"Fetched {len(df)} records from Fingrid API\")\n",
    "            return df\n",
    "        else:\n",
    "            print(f\"Warning: No 'data' field in response: {list(data.keys())}\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def fetch_weather_data(latitude, longitude, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch historical weather data from Open-Meteo API.\n",
    "    Free API - no key required!\n",
    "    \n",
    "    Args:\n",
    "        latitude: Location latitude\n",
    "        longitude: Location longitude\n",
    "        start_date: Start date (datetime)\n",
    "        end_date: End date (datetime)\n",
    "    \n",
    "    Returns:\n",
    "        pandas DataFrame with weather variables\n",
    "    \"\"\"\n",
    "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    \n",
    "    # Format dates for API\n",
    "    start_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "    end_str = end_date.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # Weather variables optimized for energy forecasting\n",
    "    params = {\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"start_date\": start_str,\n",
    "        \"end_date\": end_str,\n",
    "        \"hourly\": [\n",
    "            \"temperature_2m\",\n",
    "            \"precipitation\",\n",
    "            \"cloud_cover\",\n",
    "            \"wind_speed_10m\",\n",
    "            \"wind_speed_100m\",\n",
    "            \"wind_direction_10m\",\n",
    "            \"surface_pressure\",\n",
    "            \"shortwave_radiation\"\n",
    "        ],\n",
    "        \"timezone\": \"Europe/Helsinki\"\n",
    "    }\n",
    "    \n",
    "    print(f\"Fetching weather data from {start_str} to {end_str}...\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        if 'hourly' in data:\n",
    "            # Convert to DataFrame\n",
    "            hourly = data['hourly']\n",
    "            df = pd.DataFrame({\n",
    "                'date': pd.to_datetime(hourly['time']),\n",
    "                'temperature_2m': hourly['temperature_2m'],\n",
    "                'precipitation': hourly['precipitation'],\n",
    "                'cloud_cover': hourly['cloud_cover'],\n",
    "                'wind_speed_10m': hourly['wind_speed_10m'],\n",
    "                'wind_speed_100m': hourly['wind_speed_100m'],\n",
    "                'wind_direction_10m': hourly['wind_direction_10m'],\n",
    "                'surface_pressure': hourly['surface_pressure'],\n",
    "                'shortwave_radiation': hourly['shortwave_radiation']\n",
    "            })\n",
    "            \n",
    "            print(f\"Fetched {len(df)} hourly weather records\")\n",
    "            return df\n",
    "        else:\n",
    "            print(f\"Warning: No 'hourly' field in response: {list(data.keys())}\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching weather data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "print(\"Data fetching functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2541a0",
   "metadata": {},
   "source": [
    "### <span style='color:#ff5f27'> Fetch Energy Data from Fingrid API</span>\n",
    "\n",
    "We fetch data in chunks to respect API rate limits and avoid timeouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7ad7abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching energy data from 2024-01-02T20:37:40Z to 2024-01-09T20:37:40Z...\n",
      "Fetched 3360 records from Fingrid API\n",
      "Fetching energy data from 2024-01-09T20:37:40Z to 2024-01-16T20:37:40Z...\n",
      "Fetched 6419 records from Fingrid API\n",
      "Fetching energy data from 2024-01-16T20:37:40Z to 2024-01-23T20:37:40Z...\n",
      "Fetched 6678 records from Fingrid API\n",
      "Fetching energy data from 2024-01-23T20:37:40Z to 2024-01-30T20:37:40Z...\n",
      "Fetched 6718 records from Fingrid API\n",
      "Fetching energy data from 2024-01-30T20:37:40Z to 2024-02-06T20:37:40Z...\n",
      "Fetched 6712 records from Fingrid API\n",
      "Fetching energy data from 2024-02-06T20:37:40Z to 2024-02-13T20:37:40Z...\n",
      "Fetched 6716 records from Fingrid API\n",
      "Fetching energy data from 2024-02-13T20:37:40Z to 2024-02-20T20:37:40Z...\n",
      "Fetched 6664 records from Fingrid API\n",
      "Fetching energy data from 2024-02-20T20:37:40Z to 2024-02-27T20:37:40Z...\n",
      "Fetched 6716 records from Fingrid API\n",
      "Fetching energy data from 2024-02-27T20:37:40Z to 2024-03-05T20:37:40Z...\n",
      "Fetched 6554 records from Fingrid API\n",
      "Fetching energy data from 2024-03-05T20:37:40Z to 2024-03-12T20:37:40Z...\n",
      "Fetched 3286 records from Fingrid API\n",
      "Fetching energy data from 2024-03-12T20:37:40Z to 2024-03-19T20:37:40Z...\n",
      "Fetched 3426 records from Fingrid API\n",
      "Fetching energy data from 2024-03-19T20:37:40Z to 2024-03-26T20:37:40Z...\n",
      "Fetched 4704 records from Fingrid API\n",
      "Fetching energy data from 2024-03-26T20:37:40Z to 2024-04-02T20:37:40Z...\n",
      "Fetched 4704 records from Fingrid API\n",
      "Fetching energy data from 2024-04-02T20:37:40Z to 2024-04-09T20:37:40Z...\n",
      "Fetched 4704 records from Fingrid API\n",
      "Fetching energy data from 2024-04-09T20:37:40Z to 2024-04-16T20:37:40Z...\n",
      "Fetched 4809 records from Fingrid API\n",
      "Fetching energy data from 2024-04-16T20:37:40Z to 2024-04-23T20:37:40Z...\n",
      "Fetched 5346 records from Fingrid API\n",
      "Fetching energy data from 2024-04-23T20:37:40Z to 2024-04-30T20:37:40Z...\n",
      "Fetched 3359 records from Fingrid API\n",
      "Fetching energy data from 2024-04-30T20:37:40Z to 2024-05-07T20:37:40Z...\n",
      "Fetched 3359 records from Fingrid API\n",
      "Fetching energy data from 2024-05-07T20:37:40Z to 2024-05-14T20:37:40Z...\n",
      "Fetched 3337 records from Fingrid API\n",
      "Fetching energy data from 2024-05-14T20:37:40Z to 2024-05-21T20:37:40Z...\n",
      "Fetched 3355 records from Fingrid API\n",
      "Fetching energy data from 2024-05-21T20:37:40Z to 2024-05-28T20:37:40Z...\n",
      "Fetched 3359 records from Fingrid API\n",
      "Fetching energy data from 2024-05-28T20:37:40Z to 2024-06-04T20:37:40Z...\n",
      "Fetched 3359 records from Fingrid API\n",
      "Fetching energy data from 2024-06-04T20:37:40Z to 2024-06-11T20:37:40Z...\n",
      "Fetched 3359 records from Fingrid API\n",
      "Fetching energy data from 2024-06-11T20:37:40Z to 2024-06-18T20:37:40Z...\n",
      "Fetched 3358 records from Fingrid API\n",
      "Fetching energy data from 2024-06-18T20:37:40Z to 2024-06-25T20:37:40Z...\n",
      "Fetched 3359 records from Fingrid API\n",
      "Fetching energy data from 2024-06-25T20:37:40Z to 2024-07-02T20:37:40Z...\n",
      "Fetched 3359 records from Fingrid API\n",
      "Fetching energy data from 2024-07-02T20:37:40Z to 2024-07-09T20:37:40Z...\n",
      "Fetched 3359 records from Fingrid API\n",
      "Fetching energy data from 2024-07-09T20:37:40Z to 2024-07-16T20:37:40Z...\n",
      "Fetched 3033 records from Fingrid API\n",
      "Fetching energy data from 2024-07-16T20:37:40Z to 2024-07-23T20:37:40Z...\n",
      "Fetched 3359 records from Fingrid API\n",
      "Fetching energy data from 2024-07-23T20:37:40Z to 2024-07-30T20:37:40Z...\n",
      "Fetched 3359 records from Fingrid API\n",
      "Fetching energy data from 2024-07-30T20:37:40Z to 2024-08-06T20:37:40Z...\n",
      "Fetched 3359 records from Fingrid API\n",
      "Fetching energy data from 2024-08-06T20:37:40Z to 2024-08-13T20:37:40Z...\n",
      "Fetched 3359 records from Fingrid API\n",
      "Fetching energy data from 2024-08-13T20:37:40Z to 2024-08-20T20:37:40Z...\n",
      "Fetched 3356 records from Fingrid API\n",
      "Fetching energy data from 2024-08-20T20:37:40Z to 2024-08-27T20:37:40Z...\n",
      "Fetched 3359 records from Fingrid API\n",
      "Fetching energy data from 2024-08-27T20:37:40Z to 2024-09-03T20:37:40Z...\n",
      "Fetched 3358 records from Fingrid API\n",
      "Fetching energy data from 2024-09-03T20:37:40Z to 2024-09-10T20:37:40Z...\n",
      "Fetched 3359 records from Fingrid API\n",
      "Fetching energy data from 2024-09-10T20:37:40Z to 2024-09-17T20:37:40Z...\n",
      "Fetched 3335 records from Fingrid API\n",
      "Fetching energy data from 2024-09-17T20:37:40Z to 2024-09-24T20:37:40Z...\n",
      "Fetched 3359 records from Fingrid API\n",
      "Fetching energy data from 2024-09-24T20:37:40Z to 2024-10-01T20:37:40Z...\n",
      "Fetched 3358 records from Fingrid API\n",
      "Fetching energy data from 2024-10-01T20:37:40Z to 2024-10-08T20:37:40Z...\n",
      "Fetched 3359 records from Fingrid API\n",
      "Fetching energy data from 2024-10-08T20:37:40Z to 2024-10-15T20:37:40Z...\n",
      "Fetched 3302 records from Fingrid API\n",
      "Fetching energy data from 2024-10-15T20:37:40Z to 2024-10-22T20:37:40Z...\n",
      "Fetched 3047 records from Fingrid API\n",
      "Fetching energy data from 2024-10-22T20:37:40Z to 2024-10-29T20:37:40Z...\n",
      "Fetched 3357 records from Fingrid API\n",
      "Fetching energy data from 2024-10-29T20:37:40Z to 2024-11-05T20:37:40Z...\n",
      "Fetched 3359 records from Fingrid API\n",
      "Fetching energy data from 2024-11-05T20:37:40Z to 2024-11-12T20:37:40Z...\n",
      "Fetched 3359 records from Fingrid API\n",
      "Fetching energy data from 2024-11-12T20:37:40Z to 2024-11-19T20:37:40Z...\n",
      "Fetched 3355 records from Fingrid API\n",
      "Fetching energy data from 2024-11-19T20:37:40Z to 2024-11-26T20:37:40Z...\n",
      "Fetched 3359 records from Fingrid API\n",
      "Fetching energy data from 2024-11-26T20:37:40Z to 2024-12-03T20:37:40Z...\n",
      "Fetched 3359 records from Fingrid API\n",
      "Fetching energy data from 2024-12-03T20:37:40Z to 2024-12-10T20:37:40Z...\n",
      "Fetched 3359 records from Fingrid API\n",
      "Fetching energy data from 2024-12-10T20:37:40Z to 2024-12-17T20:37:40Z...\n",
      "Fetched 3357 records from Fingrid API\n",
      "Fetching energy data from 2024-12-17T20:37:40Z to 2024-12-24T20:37:40Z...\n",
      "Fetched 3359 records from Fingrid API\n",
      "Fetching energy data from 2024-12-24T20:37:40Z to 2024-12-31T20:37:40Z...\n",
      "Fetched 3359 records from Fingrid API\n",
      "Fetching energy data from 2024-12-31T20:37:40Z to 2025-01-07T20:37:40Z...\n",
      "Fetched 3359 records from Fingrid API\n",
      "Fetching energy data from 2025-01-07T20:37:40Z to 2025-01-14T20:37:40Z...\n",
      "Fetched 3359 records from Fingrid API\n",
      "Fetching energy data from 2025-01-14T20:37:40Z to 2025-01-21T20:37:40Z...\n",
      "Fetched 3351 records from Fingrid API\n",
      "Fetching energy data from 2025-01-21T20:37:40Z to 2025-01-28T20:37:40Z...\n",
      "Fetched 3359 records from Fingrid API\n",
      "Fetching energy data from 2025-01-28T20:37:40Z to 2025-02-04T20:37:40Z...\n",
      "Fetched 3359 records from Fingrid API\n",
      "Fetching energy data from 2025-02-04T20:37:40Z to 2025-02-11T20:37:40Z...\n",
      "Fetched 3359 records from Fingrid API\n",
      "Fetching energy data from 2025-02-11T20:37:40Z to 2025-02-18T20:37:40Z...\n",
      "Fetched 3354 records from Fingrid API\n",
      "Fetching energy data from 2025-02-18T20:37:40Z to 2025-02-25T20:37:40Z...\n",
      "Fetched 3359 records from Fingrid API\n",
      "Fetching energy data from 2025-02-25T20:37:40Z to 2025-03-04T20:37:40Z...\n",
      "Fetched 3359 records from Fingrid API\n",
      "Fetching energy data from 2025-03-04T20:37:40Z to 2025-03-11T20:37:40Z...\n",
      "Fetched 3359 records from Fingrid API\n",
      "Fetching energy data from 2025-03-11T20:37:40Z to 2025-03-18T20:37:40Z...\n",
      "Fetched 3356 records from Fingrid API\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# Add delay to respect rate limits\u001b[39;00m\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m current_start < end_date:\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Combine all chunks\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m all_data:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Fetch data in chunks if needed (Fingrid API may have limits)\n",
    "all_data = []\n",
    "current_start = start_date\n",
    "chunk_days = 7  # Fetch 7 days at a time\n",
    "\n",
    "while current_start < end_date:\n",
    "    current_end = min(current_start + timedelta(days=chunk_days), end_date)\n",
    "    \n",
    "    df_chunk = fetch_fingrid_data(\n",
    "        dataset_id=DATASET_ID,\n",
    "        start_date=current_start,\n",
    "        end_date=current_end,\n",
    "        api_key=FINGRID_API_KEY\n",
    "    )\n",
    "    \n",
    "    if not df_chunk.empty:\n",
    "        all_data.append(df_chunk)\n",
    "    \n",
    "    current_start = current_end\n",
    "    \n",
    "    # Add delay to respect rate limits\n",
    "    if current_start < end_date:\n",
    "        time.sleep(2)\n",
    "\n",
    "# Combine all chunks\n",
    "if all_data:\n",
    "    df_raw = pd.concat(all_data, ignore_index=True)\n",
    "    print(f\"\\nTotal records fetched: {len(df_raw)}\")\n",
    "    df_raw.head()\n",
    "else:\n",
    "    raise ValueError(\"No data fetched. Check API key and date range.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15cc48e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <span style='color:#ff5f27'> STEP 3: Download Historical Weather Data </span>\n",
    "\n",
    "We download historical weather data for Helsinki (representative location for Finland) using the free Open-Meteo API.\n",
    "\n",
    "The weather features we download are optimized for energy forecasting:\n",
    " * `temperature` - impacts heating/cooling demand\n",
    " * `precipitation` - affects hydropower generation\n",
    " * `wind speed` - important for wind power generation\n",
    " * `solar radiation` - affects solar power generation\n",
    " * `cloud cover` - impacts solar generation\n",
    " * `surface pressure` - general weather indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f25efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching weather data from 2024-01-02 to 2026-01-01...\n",
      "Fetched 17544 hourly weather records\n",
      "\n",
      "Total weather records fetched: 17544\n",
      "Date range: 2024-01-02 00:00:00 to 2026-01-01 23:00:00\n"
     ]
    }
   ],
   "source": [
    "# Fetch weather data\n",
    "df_weather_raw = fetch_weather_data(\n",
    "    latitude=LATITUDE,\n",
    "    longitude=LONGITUDE,\n",
    "    start_date=start_date,\n",
    "    end_date=end_date\n",
    ")\n",
    "\n",
    "if not df_weather_raw.empty:\n",
    "    print(f\"\\nTotal weather records fetched: {len(df_weather_raw)}\")\n",
    "    print(f\"Date range: {df_weather_raw['date'].min()} to {df_weather_raw['date'].max()}\")\n",
    "    df_weather_raw.head()\n",
    "else:\n",
    "    raise ValueError(\"No weather data fetched. Check date range and network connection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e23da6c",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> STEP 4: Process Weather Data </span>\n",
    "\n",
    "We resample the hourly weather data to 3-hour intervals to match the frequency of the energy consumption data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafec824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather data processed: 5848 records (3-hour intervals)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>temperature_2m</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>cloud_cover</th>\n",
       "      <th>wind_speed_10m</th>\n",
       "      <th>wind_speed_100m</th>\n",
       "      <th>wind_direction_10m</th>\n",
       "      <th>surface_pressure</th>\n",
       "      <th>shortwave_radiation</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-02 00:00:00</td>\n",
       "      <td>-15.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>25.233333</td>\n",
       "      <td>38.633333</td>\n",
       "      <td>37.333333</td>\n",
       "      <td>1022.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Finland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-02 03:00:00</td>\n",
       "      <td>-15.233333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>26.666667</td>\n",
       "      <td>40.900000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1021.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Finland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-02 06:00:00</td>\n",
       "      <td>-15.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.333333</td>\n",
       "      <td>25.966667</td>\n",
       "      <td>39.800000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>1021.866667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Finland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-02 09:00:00</td>\n",
       "      <td>-15.833333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>24.033333</td>\n",
       "      <td>38.266667</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>1022.500000</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>Finland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-02 12:00:00</td>\n",
       "      <td>-15.133333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>22.866667</td>\n",
       "      <td>36.400000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>1022.433333</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>Finland</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date  temperature_2m  precipitation  cloud_cover  \\\n",
       "0 2024-01-02 00:00:00      -15.666667            0.0   100.000000   \n",
       "1 2024-01-02 03:00:00      -15.233333            0.0   100.000000   \n",
       "2 2024-01-02 06:00:00      -15.066667            0.0    99.333333   \n",
       "3 2024-01-02 09:00:00      -15.833333            0.0   100.000000   \n",
       "4 2024-01-02 12:00:00      -15.133333            0.0   100.000000   \n",
       "\n",
       "   wind_speed_10m  wind_speed_100m  wind_direction_10m  surface_pressure  \\\n",
       "0       25.233333        38.633333           37.333333       1022.100000   \n",
       "1       26.666667        40.900000           41.000000       1021.900000   \n",
       "2       25.966667        39.800000           42.000000       1021.866667   \n",
       "3       24.033333        38.266667           39.000000       1022.500000   \n",
       "4       22.866667        36.400000           39.000000       1022.433333   \n",
       "\n",
       "   shortwave_radiation  country  \n",
       "0             0.000000  Finland  \n",
       "1             0.000000  Finland  \n",
       "2             0.000000  Finland  \n",
       "3             8.666667  Finland  \n",
       "4            41.000000  Finland  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_weather_data(df):\n",
    "    \"\"\"\n",
    "    Process weather data to match energy consumption frequency.\n",
    "    Resample hourly data to 3-hour intervals to match Fingrid data.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Set date as index for resampling\n",
    "    df.set_index('date', inplace=True)\n",
    "    \n",
    "    # Resample to 3-hour intervals\n",
    "    df_resampled = df.resample('3H').agg({\n",
    "        'temperature_2m': 'mean',\n",
    "        'precipitation': 'sum',\n",
    "        'cloud_cover': 'mean',\n",
    "        'wind_speed_10m': 'mean',\n",
    "        'wind_speed_100m': 'mean',\n",
    "        'wind_direction_10m': 'mean',\n",
    "        'surface_pressure': 'mean',\n",
    "        'shortwave_radiation': 'mean'\n",
    "    })\n",
    "    \n",
    "    # Reset index to make date a column\n",
    "    df_resampled.reset_index(inplace=True)\n",
    "    \n",
    "    # Add country identifier\n",
    "    df_resampled['country'] = COUNTRY\n",
    "    \n",
    "    # Drop any NaN rows\n",
    "    df_resampled = df_resampled.dropna()\n",
    "    \n",
    "    return df_resampled\n",
    "\n",
    "# Process the weather data\n",
    "df_weather_processed = process_weather_data(df_weather_raw)\n",
    "\n",
    "print(f\"Weather data processed: {len(df_weather_processed)} records (3-hour intervals)\")\n",
    "df_weather_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831c4a02",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <span style='color:#ff5f27'> STEP 5: Process Energy Consumption Data </span>\n",
    "\n",
    "We process the raw energy data and add temporal features to help with forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65399117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processed: 383037 records\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>consumption_mw</th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>week_of_year</th>\n",
       "      <th>consumption_lag_1</th>\n",
       "      <th>consumption_lag_2</th>\n",
       "      <th>consumption_lag_8</th>\n",
       "      <th>consumption_rolling_mean_24h</th>\n",
       "      <th>consumption_rolling_std_24h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3359</th>\n",
       "      <td>2024-01-02 20:22:00+00:00</td>\n",
       "      <td>14297.0</td>\n",
       "      <td>Finland</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14297.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3358</th>\n",
       "      <td>2024-01-02 20:25:00+00:00</td>\n",
       "      <td>14217.0</td>\n",
       "      <td>Finland</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14297.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14257.000000</td>\n",
       "      <td>56.568542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3357</th>\n",
       "      <td>2024-01-02 20:28:00+00:00</td>\n",
       "      <td>14255.0</td>\n",
       "      <td>Finland</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14217.0</td>\n",
       "      <td>14297.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14256.333333</td>\n",
       "      <td>40.016663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3356</th>\n",
       "      <td>2024-01-02 20:31:00+00:00</td>\n",
       "      <td>14285.0</td>\n",
       "      <td>Finland</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14255.0</td>\n",
       "      <td>14217.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14263.500000</td>\n",
       "      <td>35.679126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3355</th>\n",
       "      <td>2024-01-02 20:34:00+00:00</td>\n",
       "      <td>14300.0</td>\n",
       "      <td>Finland</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14285.0</td>\n",
       "      <td>14255.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14270.800000</td>\n",
       "      <td>34.945672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          date  consumption_mw  country  year  month  day  \\\n",
       "3359 2024-01-02 20:22:00+00:00         14297.0  Finland  2024      1    2   \n",
       "3358 2024-01-02 20:25:00+00:00         14217.0  Finland  2024      1    2   \n",
       "3357 2024-01-02 20:28:00+00:00         14255.0  Finland  2024      1    2   \n",
       "3356 2024-01-02 20:31:00+00:00         14285.0  Finland  2024      1    2   \n",
       "3355 2024-01-02 20:34:00+00:00         14300.0  Finland  2024      1    2   \n",
       "\n",
       "      hour  day_of_week  is_weekend  week_of_year  consumption_lag_1  \\\n",
       "3359    20            1           0             1                NaN   \n",
       "3358    20            1           0             1            14297.0   \n",
       "3357    20            1           0             1            14217.0   \n",
       "3356    20            1           0             1            14255.0   \n",
       "3355    20            1           0             1            14285.0   \n",
       "\n",
       "      consumption_lag_2  consumption_lag_8  consumption_rolling_mean_24h  \\\n",
       "3359                NaN                NaN                  14297.000000   \n",
       "3358                NaN                NaN                  14257.000000   \n",
       "3357            14297.0                NaN                  14256.333333   \n",
       "3356            14217.0                NaN                  14263.500000   \n",
       "3355            14255.0                NaN                  14270.800000   \n",
       "\n",
       "      consumption_rolling_std_24h  \n",
       "3359                          NaN  \n",
       "3358                    56.568542  \n",
       "3357                    40.016663  \n",
       "3356                    35.679126  \n",
       "3355                    34.945672  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_energy_data(df):\n",
    "    \"\"\"\n",
    "    Process and engineer features from raw Fingrid data.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Parse timestamp\n",
    "    timestamp_col = 'start_time' if 'start_time' in df.columns else 'startTime'\n",
    "    if timestamp_col not in df.columns:\n",
    "        timestamp_col = df.columns[0]\n",
    "    \n",
    "    df['date'] = pd.to_datetime(df[timestamp_col])\n",
    "    \n",
    "    # Get consumption value\n",
    "    value_col = 'value' if 'value' in df.columns else df.columns[1]\n",
    "    df['consumption_mw'] = pd.to_numeric(df[value_col], errors='coerce')\n",
    "    \n",
    "    # Remove duplicates and sort\n",
    "    df = df.drop_duplicates(subset=['date'])\n",
    "    df = df.sort_values('date')\n",
    "    \n",
    "    # Extract temporal features\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df['day_of_week'] = df['date'].dt.dayofweek\n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "    df['week_of_year'] = df['date'].dt.isocalendar().week\n",
    "    \n",
    "    # Create lag features\n",
    "    df['consumption_lag_1'] = df['consumption_mw'].shift(1)\n",
    "    df['consumption_lag_2'] = df['consumption_mw'].shift(2)\n",
    "    df['consumption_lag_8'] = df['consumption_mw'].shift(8)\n",
    "    \n",
    "    # Rolling statistics\n",
    "    df['consumption_rolling_mean_24h'] = df['consumption_mw'].rolling(window=8, min_periods=1).mean()\n",
    "    df['consumption_rolling_std_24h'] = df['consumption_mw'].rolling(window=8, min_periods=1).std()\n",
    "    \n",
    "    # Drop rows with NaN\n",
    "    df = df.dropna(subset=['date', 'consumption_mw'])\n",
    "    \n",
    "    # Add country identifier\n",
    "    df['country'] = COUNTRY\n",
    "    \n",
    "    # Select final columns\n",
    "    feature_columns = [\n",
    "        'date',\n",
    "        'consumption_mw',\n",
    "        'country',\n",
    "        'year',\n",
    "        'month',\n",
    "        'day',\n",
    "        'hour',\n",
    "        'day_of_week',\n",
    "        'is_weekend',\n",
    "        'week_of_year',\n",
    "        'consumption_lag_1',\n",
    "        'consumption_lag_2',\n",
    "        'consumption_lag_8',\n",
    "        'consumption_rolling_mean_24h',\n",
    "        'consumption_rolling_std_24h'\n",
    "    ]\n",
    "    \n",
    "    df = df[feature_columns]\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_processed = process_energy_data(df_raw)\n",
    "\n",
    "print(f\"Data processed: {len(df_processed)} records\")\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cf50bc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <span style='color:#ff5f27'> STEP 6: Define Data Validation Rules </span>\n",
    "\n",
    "We define data validation rules (expectations) to ensure data quality before writing to Hopsworks.\n",
    "This prevents garbage-in, garbage-out scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de466bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"expectation_type\": \"expect_column_min_to_be_between\", \"kwargs\": {\"column\": \"consumption_mw\", \"min_value\": 0.0, \"max_value\": 20000.0, \"strict_min\": true}, \"meta\": {}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import great_expectations as ge\n",
    "\n",
    "energy_expectation_suite = ge.core.ExpectationSuite(\n",
    "    expectation_suite_name=\"energy_expectation_suite\"\n",
    ")\n",
    "\n",
    "energy_expectation_suite.add_expectation(\n",
    "    ge.core.ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_min_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\": \"consumption_mw\",\n",
    "            \"min_value\": 0.0,\n",
    "            \"max_value\": 20000.0,\n",
    "            \"strict_min\": True\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524498d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_expectation_suite = ge.core.ExpectationSuite(\n",
    "    expectation_suite_name=\"weather_expectation_suite\"\n",
    ")\n",
    "\n",
    "def expect_reasonable_weather(col, min_val, max_val):\n",
    "    weather_expectation_suite.add_expectation(\n",
    "        ge.core.ExpectationConfiguration(\n",
    "            expectation_type=\"expect_column_min_to_be_between\",\n",
    "            kwargs={\n",
    "                \"column\": col,\n",
    "                \"min_value\": min_val,\n",
    "                \"max_value\": max_val,\n",
    "                \"strict_min\": True\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "expect_reasonable_weather(\"temperature_2m\", -50.0, 50.0)\n",
    "expect_reasonable_weather(\"precipitation\", 0.0, 500.0)\n",
    "expect_reasonable_weather(\"wind_speed_10m\", 0.0, 100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71f036a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <span style='color:#ff5f27'> STEP 7: Save Processed Data Locally </span>\n",
    "\n",
    "Before uploading to Hopsworks, save the processed dataframes as CSV files in the data directory for backup and offline analysis.\n",
    "\n",
    "The files will be saved as:\n",
    " * `energy_consumption_finland.csv` - Energy consumption with features (2 years)\n",
    " * `weather_finland_historical.csv` - Historical weather data (2 years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a828e718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved energy data to: c:\\Users\\patri\\VScodeProjects\\25-ID2223-mlfs-book/data/energy_consumption_finland.csv\n",
      "  - 383037 records from 2024-01-02 20:22:00+00:00 to 2026-01-01 19:21:00+00:00\n",
      "✓ Saved weather data to: c:\\Users\\patri\\VScodeProjects\\25-ID2223-mlfs-book/data/weather_finland_historical.csv\n",
      "  - 5848 records from 2024-01-02 00:00:00 to 2026-01-01 21:00:00\n",
      "\n",
      "✓ All data saved successfully in: c:\\Users\\patri\\VScodeProjects\\25-ID2223-mlfs-book/data\n"
     ]
    }
   ],
   "source": [
    "# Save processed data to CSV files in the data directory\n",
    "# Using unique names to avoid conflicts with other project data\n",
    "energy_file = f\"{DATA_DIR}/energy_consumption_finland.csv\"\n",
    "weather_file = f\"{DATA_DIR}/weather_finland_historical.csv\"\n",
    "\n",
    "df_processed.to_csv(energy_file, index=False)\n",
    "df_weather_processed.to_csv(weather_file, index=False)\n",
    "\n",
    "print(f\"✓ Saved energy data to: {energy_file}\")\n",
    "print(f\"  - {len(df_processed)} records from {df_processed['date'].min()} to {df_processed['date'].max()}\")\n",
    "print(f\"✓ Saved weather data to: {weather_file}\")\n",
    "print(f\"  - {len(df_weather_processed)} records from {df_weather_processed['date'].min()} to {df_weather_processed['date'].max()}\")\n",
    "print(f\"\\n✓ All data saved successfully in: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01b6897",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <span style=\"color:#ff5f27;\"> STEP 8: Connect to Hopsworks Feature Store</span>\n",
    "\n",
    "Now we'll connect to Hopsworks and upload the processed data to feature groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3a6285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Hopsworks Feature Store: id2223_10_featurestore\n"
     ]
    }
   ],
   "source": [
    "fs = project.get_feature_store() \n",
    "\n",
    "try:\n",
    "    secrets = project.get_secrets_api()\n",
    "except AttributeError:\n",
    "    try:\n",
    "        secrets = project.get_secret_store()\n",
    "    except AttributeError:\n",
    "        secrets = hopsworks.get_secrets_api()\n",
    "\n",
    "print(f\"Connected to Hopsworks Feature Store: {fs.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0385f729",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> STEP 9: Create Feature Groups and Upload Data </span>\n",
    "\n",
    "### <span style='color:#ff5f27'> Energy Consumption Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302a0e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_fg = fs.get_or_create_feature_group(\n",
    "    name=\"energy_consumption\",\n",
    "    version=1,\n",
    "    description=\"Historical electricity consumption data from Fingrid with temporal and lag features\",\n",
    "    primary_key=['country'],\n",
    "    event_time='date',\n",
    "    expectation_suite=energy_expectation_suite\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e17f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1286359/fs/1284182/fg/1876584\n",
      "2026-01-01 20:26:36,248 INFO: \t1 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1286359/fs/1284182/fg/1876584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 383037/383037 | Elapsed Time: 00:14 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: energy_consumption_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1286359/jobs/named/energy_consumption_1_offline_fg_materialization/executions\n",
      "2026-01-01 20:27:06,767 INFO: Waiting for execution to finish. Current state: SUBMITTED. Final status: UNDEFINED\n",
      "2026-01-01 20:27:09,939 INFO: Waiting for execution to finish. Current state: RUNNING. Final status: UNDEFINED\n",
      "2026-01-01 20:29:33,365 INFO: Waiting for execution to finish. Current state: AGGREGATING_LOGS. Final status: SUCCEEDED\n",
      "2026-01-01 20:29:33,534 INFO: Waiting for log aggregation to finish.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43menergy_fg\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_processed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwait_for_job\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\patri\\miniconda3\\envs\\mlfs-book\\Lib\\site-packages\\hsfs\\feature_group.py:3153\u001b[39m, in \u001b[36mFeatureGroup.insert\u001b[39m\u001b[34m(self, features, overwrite, operation, storage, write_options, validation_options, wait, transformation_context, transform)\u001b[39m\n\u001b[32m   3150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._id \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._offline_backfill_every_hr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3151\u001b[39m     write_options[\u001b[33m\"\u001b[39m\u001b[33moffline_backfill_every_hr\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m._offline_backfill_every_hr\n\u001b[32m-> \u001b[39m\u001b[32m3153\u001b[39m job, ge_report = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_feature_group_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3154\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3155\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_dataframe\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_dataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3156\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3157\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3158\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   3159\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3160\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msave_report\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mvalidation_options\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3161\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformation_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformation_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3162\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3163\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine.get_type().startswith(\u001b[33m\"\u001b[39m\u001b[33mspark\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream:\n\u001b[32m   3166\u001b[39m     \u001b[38;5;66;03m# Also, only compute statistics if stream is False.\u001b[39;00m\n\u001b[32m   3167\u001b[39m     \u001b[38;5;66;03m# if True, the backfill job has not been triggered and the data has not been inserted (it's in Kafka)\u001b[39;00m\n\u001b[32m   3168\u001b[39m     \u001b[38;5;28mself\u001b[39m.compute_statistics()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\patri\\miniconda3\\envs\\mlfs-book\\Lib\\site-packages\\hsfs\\core\\feature_group_engine.py:245\u001b[39m, in \u001b[36mFeatureGroupEngine.insert\u001b[39m\u001b[34m(self, feature_group, feature_dataframe, overwrite, operation, storage, write_options, validation_options, transformation_context, transform)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m overwrite:\n\u001b[32m    242\u001b[39m     \u001b[38;5;28mself\u001b[39m._feature_group_api.delete_content(feature_group)\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m     \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_dataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbulk_insert\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m.\u001b[49m\u001b[43monline_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffline_write_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43monline_write_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    254\u001b[39m     ge_report,\n\u001b[32m    255\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\patri\\miniconda3\\envs\\mlfs-book\\Lib\\site-packages\\hsfs\\engine\\python.py:819\u001b[39m, in \u001b[36mEngine.save_dataframe\u001b[39m\u001b[34m(self, feature_group, dataframe, operation, online_enabled, storage, offline_write_options, online_write_options, validation_id)\u001b[39m\n\u001b[32m    804\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_dataframe\u001b[39m(\n\u001b[32m    805\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    806\u001b[39m     feature_group: FeatureGroup,\n\u001b[32m   (...)\u001b[39m\u001b[32m    813\u001b[39m     validation_id: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    814\u001b[39m ) -> Optional[job.Job]:\n\u001b[32m    815\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    816\u001b[39m         \u001b[38;5;28mhasattr\u001b[39m(feature_group, \u001b[33m\"\u001b[39m\u001b[33mEXTERNAL_FEATURE_GROUP\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    817\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m feature_group.online_enabled\n\u001b[32m    818\u001b[39m     ) \u001b[38;5;129;01mor\u001b[39;00m feature_group.stream:\n\u001b[32m--> \u001b[39m\u001b[32m819\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_write_dataframe_kafka\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    820\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffline_write_options\u001b[49m\n\u001b[32m    821\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    822\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    823\u001b[39m         \u001b[38;5;66;03m# for backwards compatibility\u001b[39;00m\n\u001b[32m    824\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.legacy_save_dataframe(\n\u001b[32m    825\u001b[39m             feature_group,\n\u001b[32m    826\u001b[39m             dataframe,\n\u001b[32m   (...)\u001b[39m\u001b[32m    832\u001b[39m             validation_id,\n\u001b[32m    833\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\patri\\miniconda3\\envs\\mlfs-book\\Lib\\site-packages\\hsfs\\engine\\python.py:1605\u001b[39m, in \u001b[36mEngine._write_dataframe_kafka\u001b[39m\u001b[34m(self, feature_group, dataframe, offline_write_options)\u001b[39m\n\u001b[32m   1603\u001b[39m         initial_check_point = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1604\u001b[39m     \u001b[38;5;66;03m# provide the initial_check_point as it will reduce the read amplification of materialization job\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1605\u001b[39m     \u001b[43mfeature_group\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaterialization_job\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1606\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaterialization_job\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdefaultArgs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1607\u001b[39m \u001b[43m        \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1608\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m -initialCheckPointString \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43minitial_check_point\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   1609\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minitial_check_point\u001b[49m\n\u001b[32m   1610\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   1611\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1612\u001b[39m \u001b[43m        \u001b[49m\u001b[43mawait_termination\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffline_write_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwait_for_job\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1613\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1615\u001b[39m \u001b[38;5;66;03m# wait for online ingestion\u001b[39;00m\n\u001b[32m   1616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m feature_group.online_enabled \u001b[38;5;129;01mand\u001b[39;00m offline_write_options.get(\n\u001b[32m   1617\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mwait_for_online_ingestion\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1618\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\patri\\miniconda3\\envs\\mlfs-book\\Lib\\site-packages\\hopsworks_common\\usage.py:242\u001b[39m, in \u001b[36mmethod_logger.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    239\u001b[39m exception = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# Call the original method\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\patri\\miniconda3\\envs\\mlfs-book\\Lib\\site-packages\\hopsworks_common\\job.py:186\u001b[39m, in \u001b[36mJob.run\u001b[39m\u001b[34m(self, args, await_termination)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    183\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mJob started successfully, you can follow the progress at \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexecution.get_url()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    184\u001b[39m )\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m await_termination:\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait_until_finished\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m execution\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\patri\\miniconda3\\envs\\mlfs-book\\Lib\\site-packages\\hopsworks_common\\engine\\execution_engine.py:163\u001b[39m, in \u001b[36mExecutionEngine.wait_until_finished\u001b[39m\u001b[34m(self, job, execution, timeout)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m log_aggregation_files_exist_already:\n\u001b[32m    162\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m timeout \u001b[38;5;129;01mor\u001b[39;00m timeout > passed() + \u001b[32m5\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Helps for log aggregation to flush to filesystem\u001b[39;00m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_yarn_job \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m updated_execution.success:\n\u001b[32m    166\u001b[39m     \u001b[38;5;28mself\u001b[39m._log.error(\n\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mExecution failed with status: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. See the logs for more information.\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    168\u001b[39m             updated_execution.final_status\n\u001b[32m    169\u001b[39m         )\n\u001b[32m    170\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "energy_fg.insert(df_processed, write_options={\"wait_for_job\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f774dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<hsfs.feature_group.FeatureGroup at 0x165e50f9290>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "energy_fg.update_feature_description(\"date\", \"Timestamp of measurement (3-hour intervals)\")\n",
    "energy_fg.update_feature_description(\"consumption_mw\", \"Electricity consumption in Finland (megawatts)\")\n",
    "energy_fg.update_feature_description(\"country\", \"Country where consumption was measured\")\n",
    "energy_fg.update_feature_description(\"year\", \"Year extracted from date\")\n",
    "energy_fg.update_feature_description(\"month\", \"Month of year (1-12)\")\n",
    "energy_fg.update_feature_description(\"day\", \"Day of month\")\n",
    "energy_fg.update_feature_description(\"hour\", \"Hour of day\")\n",
    "energy_fg.update_feature_description(\"day_of_week\", \"Day of week (0=Monday, 6=Sunday)\")\n",
    "energy_fg.update_feature_description(\"is_weekend\", \"1 if weekend, 0 if weekday\")\n",
    "energy_fg.update_feature_description(\"week_of_year\", \"Week number in the year\")\n",
    "energy_fg.update_feature_description(\"consumption_lag_1\", \"Consumption 3 hours ago\")\n",
    "energy_fg.update_feature_description(\"consumption_lag_2\", \"Consumption 6 hours ago\")\n",
    "energy_fg.update_feature_description(\"consumption_lag_8\", \"Consumption 24 hours ago\")\n",
    "energy_fg.update_feature_description(\"consumption_rolling_mean_24h\", \"24-hour rolling mean consumption\")\n",
    "energy_fg.update_feature_description(\"consumption_rolling_std_24h\", \"24-hour rolling standard deviation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ac6d15",
   "metadata": {},
   "source": [
    "### <span style='color:#ff5f27'> Weather Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56437240",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_fg = fs.get_or_create_feature_group(\n",
    "    name=\"weather_finland\",\n",
    "    version=1,\n",
    "    description=\"Historical weather data from Open-Meteo for Finland (measured in Helsinki)\",\n",
    "    primary_key=['country'],\n",
    "    event_time='date',\n",
    "    expectation_suite=weather_expectation_suite\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19add896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1286359/fs/1284182/fg/1876571\n",
      "2026-01-01 12:52:02,466 INFO: \t3 expectation(s) included in expectation_suite.\n",
      "Validation failed.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1286359/fs/1284182/fg/1876571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 1448/1448 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: weather_finland_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1286359/jobs/named/weather_finland_1_offline_fg_materialization/executions\n",
      "2026-01-01 12:52:19,991 INFO: Waiting for execution to finish. Current state: INITIALIZING. Final status: UNDEFINED\n",
      "2026-01-01 12:52:23,155 INFO: Waiting for execution to finish. Current state: SUBMITTED. Final status: UNDEFINED\n",
      "2026-01-01 12:52:26,315 INFO: Waiting for execution to finish. Current state: RUNNING. Final status: UNDEFINED\n",
      "2026-01-01 12:54:20,145 INFO: Waiting for execution to finish. Current state: AGGREGATING_LOGS. Final status: SUCCEEDED\n",
      "2026-01-01 12:54:20,297 INFO: Waiting for log aggregation to finish.\n",
      "2026-01-01 12:54:42,171 INFO: Execution finished successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Job('weather_finland_1_offline_fg_materialization', 'SPARK'),\n",
       " {\n",
       "   \"success\": false,\n",
       "   \"results\": [\n",
       "     {\n",
       "       \"success\": true,\n",
       "       \"expectation_config\": {\n",
       "         \"expectation_type\": \"expect_column_min_to_be_between\",\n",
       "         \"kwargs\": {\n",
       "           \"column\": \"wind_speed_10m\",\n",
       "           \"min_value\": 0.0,\n",
       "           \"max_value\": 100.0,\n",
       "           \"strict_min\": true\n",
       "         },\n",
       "         \"meta\": {\n",
       "           \"expectationId\": 799816\n",
       "         }\n",
       "       },\n",
       "       \"result\": {\n",
       "         \"observed_value\": 0.7000000000000001,\n",
       "         \"element_count\": 1448,\n",
       "         \"missing_count\": null,\n",
       "         \"missing_percent\": null\n",
       "       },\n",
       "       \"meta\": {\n",
       "         \"ingestionResult\": \"INGESTED\",\n",
       "         \"validationTime\": \"2026-01-01T11:52:02.000465Z\"\n",
       "       },\n",
       "       \"exception_info\": {\n",
       "         \"raised_exception\": false,\n",
       "         \"exception_message\": null,\n",
       "         \"exception_traceback\": null\n",
       "       }\n",
       "     },\n",
       "     {\n",
       "       \"success\": true,\n",
       "       \"expectation_config\": {\n",
       "         \"expectation_type\": \"expect_column_min_to_be_between\",\n",
       "         \"kwargs\": {\n",
       "           \"column\": \"temperature_2m\",\n",
       "           \"min_value\": -50.0,\n",
       "           \"max_value\": 50.0,\n",
       "           \"strict_min\": true\n",
       "         },\n",
       "         \"meta\": {\n",
       "           \"expectationId\": 799818\n",
       "         }\n",
       "       },\n",
       "       \"result\": {\n",
       "         \"observed_value\": -15.366666666666667,\n",
       "         \"element_count\": 1448,\n",
       "         \"missing_count\": null,\n",
       "         \"missing_percent\": null\n",
       "       },\n",
       "       \"meta\": {\n",
       "         \"ingestionResult\": \"INGESTED\",\n",
       "         \"validationTime\": \"2026-01-01T11:52:02.000465Z\"\n",
       "       },\n",
       "       \"exception_info\": {\n",
       "         \"raised_exception\": false,\n",
       "         \"exception_message\": null,\n",
       "         \"exception_traceback\": null\n",
       "       }\n",
       "     },\n",
       "     {\n",
       "       \"success\": false,\n",
       "       \"expectation_config\": {\n",
       "         \"expectation_type\": \"expect_column_min_to_be_between\",\n",
       "         \"kwargs\": {\n",
       "           \"column\": \"precipitation\",\n",
       "           \"min_value\": 0.0,\n",
       "           \"max_value\": 500.0,\n",
       "           \"strict_min\": true\n",
       "         },\n",
       "         \"meta\": {\n",
       "           \"expectationId\": 799817\n",
       "         }\n",
       "       },\n",
       "       \"result\": {\n",
       "         \"observed_value\": 0.0,\n",
       "         \"element_count\": 1448,\n",
       "         \"missing_count\": null,\n",
       "         \"missing_percent\": null\n",
       "       },\n",
       "       \"meta\": {\n",
       "         \"ingestionResult\": \"INGESTED\",\n",
       "         \"validationTime\": \"2026-01-01T11:52:02.000465Z\"\n",
       "       },\n",
       "       \"exception_info\": {\n",
       "         \"raised_exception\": false,\n",
       "         \"exception_message\": null,\n",
       "         \"exception_traceback\": null\n",
       "       }\n",
       "     }\n",
       "   ],\n",
       "   \"evaluation_parameters\": {},\n",
       "   \"statistics\": {\n",
       "     \"evaluated_expectations\": 3,\n",
       "     \"successful_expectations\": 2,\n",
       "     \"unsuccessful_expectations\": 1,\n",
       "     \"success_percent\": 66.66666666666666\n",
       "   },\n",
       "   \"meta\": {\n",
       "     \"great_expectations_version\": \"0.18.12\",\n",
       "     \"expectation_suite_name\": \"weather_expectation_suite\",\n",
       "     \"run_id\": {\n",
       "       \"run_name\": null,\n",
       "       \"run_time\": \"2026-01-01T12:52:02.465037+01:00\"\n",
       "     },\n",
       "     \"batch_kwargs\": {\n",
       "       \"ge_batch_id\": \"49566d65-e708-11f0-8e49-ec691f1f0fce\"\n",
       "     },\n",
       "     \"batch_markers\": {},\n",
       "     \"batch_parameters\": {},\n",
       "     \"validation_time\": \"20260101T115202.465037Z\",\n",
       "     \"expectation_suite_meta\": {\n",
       "       \"great_expectations_version\": \"0.18.12\"\n",
       "     }\n",
       "   }\n",
       " })"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_fg.insert(df_weather_processed, write_options={\"wait_for_job\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921eec92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<hsfs.feature_group.FeatureGroup at 0x165e302ff50>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_fg.update_feature_description(\"date\", \"Timestamp of weather measurement\")\n",
    "weather_fg.update_feature_description(\"country\", \"Country where weather is measured\")\n",
    "weather_fg.update_feature_description(\"temperature_2m\", \"Temperature in Celsius at 2m height\")\n",
    "weather_fg.update_feature_description(\"precipitation\", \"Precipitation in mm (3-hour total)\")\n",
    "weather_fg.update_feature_description(\"cloud_cover\", \"Cloud cover percentage (0-100)\")\n",
    "weather_fg.update_feature_description(\"wind_speed_10m\", \"Wind speed at 10m above ground (km/h)\")\n",
    "weather_fg.update_feature_description(\"wind_speed_100m\", \"Wind speed at 100m above ground (km/h) - important for wind turbines\")\n",
    "weather_fg.update_feature_description(\"wind_direction_10m\", \"Wind direction in degrees\")\n",
    "weather_fg.update_feature_description(\"surface_pressure\", \"Surface atmospheric pressure in hPa\")\n",
    "weather_fg.update_feature_description(\"shortwave_radiation\", \"Solar radiation in W/m²\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96af3e69",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd395552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energy Consumption Feature Group:\n",
      "  Name: energy_consumption\n",
      "  Version: 1\n",
      "  Features: ['date', 'consumption_mw', 'country', 'year', 'month', 'day', 'hour', 'day_of_week', 'is_weekend', 'week_of_year', 'consumption_lag_1', 'consumption_lag_2', 'consumption_lag_8', 'consumption_rolling_mean_24h', 'consumption_rolling_std_24h']\n",
      "\n",
      "Weather Feature Group:\n",
      "  Name: weather_finland\n",
      "  Version: 1\n",
      "  Features: ['date', 'temperature_2m', 'precipitation', 'cloud_cover', 'wind_speed_10m', 'wind_speed_100m', 'wind_direction_10m', 'surface_pressure', 'shortwave_radiation', 'country']\n",
      "\n",
      "Join keys:\n",
      "  Energy FG primary keys: ['country']\n",
      "  Weather FG primary keys: ['country']\n",
      "\n",
      "✓ Both feature groups created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Verify feature groups exist and have data\n",
    "print(\"Energy Consumption Feature Group:\")\n",
    "print(f\"  Name: {energy_fg.name}\")\n",
    "print(f\"  Version: {energy_fg.version}\")\n",
    "print(f\"  Features: {[f.name for f in energy_fg.features]}\")\n",
    "\n",
    "print(\"\\nWeather Feature Group:\")\n",
    "print(f\"  Name: {weather_fg.name}\")\n",
    "print(f\"  Version: {weather_fg.version}\")\n",
    "print(f\"  Features: {[f.name for f in weather_fg.features]}\")\n",
    "\n",
    "# Check if both have 'country' as join key\n",
    "print(\"\\nJoin keys:\")\n",
    "print(f\"  Energy FG primary keys: {energy_fg.primary_key}\")\n",
    "print(f\"  Weather FG primary keys: {weather_fg.primary_key}\")\n",
    "print(\"\\n✓ Both feature groups created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407dbbad",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> Verify Feature Groups Created </span>\n",
    "\n",
    "Let's verify both feature groups were created successfully."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlfs-book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
